{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_Internal_Lab_UpdatedTF2_Prices_Iris.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "84Q8JfvaeZZ6"
      },
      "source": [
        "## Linear Classifier in TensorFlow \n",
        "Using Low Level API in Eager Execution mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sb7Epo0VOB58"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fHpCNRv1OB5-",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mjtb-EMcm5K0",
        "colab": {}
      },
      "source": [
        "#Enable Eager Execution if using tensflow version < 2.0\n",
        "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxJDmJqqOB6K"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhllFLyKOB6N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b8b948ee-56ba-4ccd-edea-7f37310c5346"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KiObW4V4SIOz",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B4yQKMiJOB6R",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/11_Iris.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDxn_fqd4cN3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data2 = pd.read_csv('/content/drive/My Drive/prices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fgkX6SEqOB6W"
      },
      "source": [
        "### Check all columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bndKQD2s4aKa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "9c20e032-6cea-4ff5-ebee-fe2ed3eddcbb"
      },
      "source": [
        "data2.info()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 851264 entries, 0 to 851263\n",
            "Data columns (total 7 columns):\n",
            "date      851264 non-null object\n",
            "symbol    851264 non-null object\n",
            "open      851264 non-null float64\n",
            "close     851264 non-null float64\n",
            "low       851264 non-null float64\n",
            "high      851264 non-null float64\n",
            "volume    851264 non-null float64\n",
            "dtypes: float64(5), object(2)\n",
            "memory usage: 45.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dU6X7MpOB6c"
      },
      "source": [
        "### Drop columns `date` and  `symbol`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lh_6spSKOB6e",
        "colab": {}
      },
      "source": [
        "data2.drop(['date','symbol'], axis=1, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xlwbUgTwOB6i",
        "outputId": "94421090-6390-4e45-f01b-374cdd577d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data2.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3DBv3WWYOB6q"
      },
      "source": [
        "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
        "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_hG9rGBOB6s",
        "colab": {}
      },
      "source": [
        "data_new = data2.iloc[:1000,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrw5ja729Q8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c98eb81b-b14e-4f74-b04f-24729899afaf"
      },
      "source": [
        "data_new.head()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak6te9uY77aC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "418a2c26-341f-4850-f3c9-2686954b0678"
      },
      "source": [
        "data_new.volume = data_new.volume.div(1000000)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py:5096: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3UaApqYOB6x"
      },
      "source": [
        "### Divide the data into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LE4U8lTdQJq",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ao-S0tQGcncz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "815d6657-1cf3-4185-bdd6-d460a8f8fbe2"
      },
      "source": [
        "data_new.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la_ps4Z2_5Ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esgsEBXHBb_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=data_new[['open','close','low','high']]\n",
        "y=data_new['volume']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDx4ul41Bre2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=.3,random_state=4)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWNgLffpAkNN",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#Convert Training and Test Datato Numpy float32 arays\n",
        "#Convert Training and Test Datato Numpy float32 arays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7725k10BFcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = np.float32(xtrain)\n",
        "xtest = np.float32(xtest)\n",
        "ytrain = np.float32(ytrain)\n",
        "ytest = np.float32(ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "im1ZegbDdKgv"
      },
      "source": [
        "### Normalize the data\n",
        "You can use Normalizer from sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2EkKAy7fOB6y",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeehdNwmDR_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain_norm = preprocessing.normalize(xtrain, norm='l2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FFfKzqtDSvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtest_norm = preprocessing.normalize(xtest, norm='l2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4OQknCAFSu0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "a7df57f8-5b7f-496f-9b09-ac78a6055860"
      },
      "source": [
        "xtrain_norm"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.4967648 , 0.5027116 , 0.4967648 , 0.50371665],\n",
              "       [0.4989744 , 0.50018555, 0.49473557, 0.5060392 ],\n",
              "       [0.49402478, 0.5091425 , 0.48619598, 0.5102223 ],\n",
              "       ...,\n",
              "       [0.5000676 , 0.49990803, 0.49790606, 0.50210947],\n",
              "       [0.50201964, 0.49833545, 0.4974043 , 0.50222206],\n",
              "       [0.4946981 , 0.5058178 , 0.49348584, 0.50585955]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v6vE4eYCOB62"
      },
      "source": [
        "## Building the Model in tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "297_qja4OB7A"
      },
      "source": [
        "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L205qPeQOB7B",
        "colab": {}
      },
      "source": [
        "x_ = tf.placeholder(dtype=tf.float32, name='x-input')\n",
        "\n",
        "x_n = tf.nn.l2_normalize(x_,1)\n",
        "\n",
        "y_ = tf.placeholder(dtype=tf.float32, name='y-input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdAv-7ynLgkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(tf.random_normal(shape=[4,1]), name=\"Weights\")\n",
        "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HgtWA-UIOB7F"
      },
      "source": [
        "2.Define a function to calculate prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JveGlx25OB7H",
        "colab": {}
      },
      "source": [
        "ys = tf.add(tf.matmul(x_n,W),b,name='output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TL1hIwf_OB7M"
      },
      "source": [
        "3.Loss (Cost) Function [Mean square error]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8VSWPiGXOB7P",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.square(ys-y_),name='Loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jzG85FUlOB7U"
      },
      "source": [
        "4.Function to train the Model\n",
        "\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cj802w-3OB7X",
        "colab": {}
      },
      "source": [
        "#Lets start graph Execution\n",
        "sess = tf.Session()\n",
        "\n",
        "# variables need to be initialized before we can use them\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcthZzLpNi3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_op = tf.train.GradientDescentOptimizer(0.03).minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSypb_u8OB7e"
      },
      "source": [
        "## Train the model for 100 epochs \n",
        "1. Observe the training loss at every iteration\n",
        "2. Observe Train loss at every 5th iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DVvgj7eQOB7f",
        "colab": {}
      },
      "source": [
        "#how many times data need to be shown to model\n",
        "training_epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HOy_bExNEym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "cd33adfd-e6a1-4764-fb6b-90c13dda8f47"
      },
      "source": [
        "for epoch in range(training_epochs):\n",
        "            \n",
        "    #Calculate train_op and loss\n",
        "    _, train_loss = sess.run([train_op,loss],feed_dict={x_:xtrain_norm, y_:ytrain})\n",
        "    \n",
        "    if epoch % 5 == 0:\n",
        "        print ('Training loss at step: ', epoch, ' is ', train_loss)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Training loss at step: ', 0, ' is ', 215.97371)\n",
            "('Training loss at step: ', 5, ' is ', 200.649)\n",
            "('Training loss at step: ', 10, ' is ', 196.3809)\n",
            "('Training loss at step: ', 15, ' is ', 195.19218)\n",
            "('Training loss at step: ', 20, ' is ', 194.86113)\n",
            "('Training loss at step: ', 25, ' is ', 194.76892)\n",
            "('Training loss at step: ', 30, ' is ', 194.74321)\n",
            "('Training loss at step: ', 35, ' is ', 194.73608)\n",
            "('Training loss at step: ', 40, ' is ', 194.73407)\n",
            "('Training loss at step: ', 45, ' is ', 194.73355)\n",
            "('Training loss at step: ', 50, ' is ', 194.73338)\n",
            "('Training loss at step: ', 55, ' is ', 194.73335)\n",
            "('Training loss at step: ', 60, ' is ', 194.73332)\n",
            "('Training loss at step: ', 65, ' is ', 194.73332)\n",
            "('Training loss at step: ', 70, ' is ', 194.73332)\n",
            "('Training loss at step: ', 75, ' is ', 194.73332)\n",
            "('Training loss at step: ', 80, ' is ', 194.73334)\n",
            "('Training loss at step: ', 85, ' is ', 194.73332)\n",
            "('Training loss at step: ', 90, ' is ', 194.73332)\n",
            "('Training loss at step: ', 95, ' is ', 194.73332)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DOL2ncA1OB7q"
      },
      "source": [
        "### Get the shapes and values of W and b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZGvtyTeuOB7r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "283c1291-3859-4bd6-84c4-98ad3614559d"
      },
      "source": [
        "print (W.shape)\n",
        "sess.run(W)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.630128  ],\n",
              "       [-0.53380036],\n",
              "       [ 2.638067  ],\n",
              "       [ 1.1360152 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vhDtOv5UOB7x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "be7b25d9-2fe7-4489-81c6-24a385da2452"
      },
      "source": [
        "print (b.shape)\n",
        "sess.run(b)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.3044198], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ERq9GOKKciho"
      },
      "source": [
        "### Model Prediction on 1st Examples in Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKGvUWahcihp",
        "colab": {}
      },
      "source": [
        "y_pred = tf.add(tf.matmul(xtest_norm[0:1],W),b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8eBsUHUXACi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9171f57d-f764-4cc5-fe8a-b7e5fa3f9a1a"
      },
      "source": [
        "sess.run(y_pred)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.2244844]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YJRBuqXhOB7_"
      },
      "source": [
        "## Classification using tf.Keras\n",
        "\n",
        "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O0g6lorycihf"
      },
      "source": [
        "### Load the given Iris data using pandas (Iris.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6xFvb5sRcihg",
        "colab": {}
      },
      "source": [
        "iris = data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az-QU0pnYlSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bfc5658e-7466-4c5a-b606-f3be0f57a6dc"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SAB--Qdwcihm"
      },
      "source": [
        "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IJr5dYnocihm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "533c5571-3b62-4aa7-b7b2-9d5e0791670b"
      },
      "source": [
        "print(iris.info())\n",
        "print(iris.head())"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 6 columns):\n",
            "Id               150 non-null int64\n",
            "SepalLengthCm    150 non-null float64\n",
            "SepalWidthCm     150 non-null float64\n",
            "PetalLengthCm    150 non-null float64\n",
            "PetalWidthCm     150 non-null float64\n",
            "Species          150 non-null object\n",
            "dtypes: float64(4), int64(1), object(1)\n",
            "memory usage: 7.1+ KB\n",
            "None\n",
            "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
            "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
            "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
            "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
            "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
            "4   5            5.0           3.6            1.4           0.2  Iris-setosa\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdPo5cqNYz-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = iris.drop(\"Id\", axis=1)\n",
        "iris_1 = iris.join(pd.get_dummies(iris[\"Species\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cbx9btVPY1aV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fc2cfe3a-b8dd-4640-82a7-4713135f869c"
      },
      "source": [
        "iris_1 = iris_1.drop(\"Species\", axis=1)\n",
        "iris_1.head()"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SepalLengthCm</th>\n",
              "      <th>SepalWidthCm</th>\n",
              "      <th>PetalLengthCm</th>\n",
              "      <th>PetalWidthCm</th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   SepalLengthCm  SepalWidthCm  ...  Iris-versicolor  Iris-virginica\n",
              "0            5.1           3.5  ...                0               0\n",
              "1            4.9           3.0  ...                0               0\n",
              "2            4.7           3.2  ...                0               0\n",
              "3            4.6           3.1  ...                0               0\n",
              "4            5.0           3.6  ...                0               0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D95nY5ILcihj"
      },
      "source": [
        "### Splitting the data into feature set and target set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RyMQoLMucihj",
        "colab": {}
      },
      "source": [
        "cols = [\"Iris-setosa\",\"Iris-versicolor\",\"Iris-virginica\"]\n",
        "feature = iris_1.drop(cols, axis=1)\n",
        "target = iris_1[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sI0brZuZG_x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "a76548b6-ef5b-4cff-d378-296fa211d2a5"
      },
      "source": [
        "print(feature.head())\n",
        "print(target.head())"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
            "0            5.1           3.5            1.4           0.2\n",
            "1            4.9           3.0            1.4           0.2\n",
            "2            4.7           3.2            1.3           0.2\n",
            "3            4.6           3.1            1.5           0.2\n",
            "4            5.0           3.6            1.4           0.2\n",
            "   Iris-setosa  Iris-versicolor  Iris-virginica\n",
            "0            1                0               0\n",
            "1            1                0               0\n",
            "2            1                0               0\n",
            "3            1                0               0\n",
            "4            1                0               0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b22qpC5xcihr"
      },
      "source": [
        "###  Building Model in tf.keras\n",
        "\n",
        "Build a Linear Classifier model  <br>\n",
        "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
        "2. Apply Softmax on Dense Layer outputs <br>\n",
        "3. Use SGD as Optimizer\n",
        "4. Use categorical_crossentropy as loss function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hov_UFnUciht",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "532bb717-f221-4a57-b165-7c251b299b59"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(3, input_dim=4, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd')"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 11:57:34.677016 140190130624384 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 11:57:34.680272 140190130624384 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 11:57:34.688694 140190130624384 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 11:57:34.710283 140190130624384 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0721 11:57:34.909998 140190130624384 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWTwPPKJZTCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size=.3, random_state=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5FdzqIKcihw"
      },
      "source": [
        "### Model Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4qLEdHPscihx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3903b145-811d-41b3-d91e-ecb8c7d272d6"
      },
      "source": [
        "hist = model.fit(X_train, y_train, epochs=150, batch_size=10)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 11:58:17.729062 140190130624384 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:1250: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0721 11:58:17.775255 140190130624384 deprecation_wrapper.py:119] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.7724\n",
            "Epoch 2/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 1.2834\n",
            "Epoch 3/150\n",
            "105/105 [==============================] - 0s 138us/step - loss: 1.1226\n",
            "Epoch 4/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 1.0392\n",
            "Epoch 5/150\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.9921\n",
            "Epoch 6/150\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.9321\n",
            "Epoch 7/150\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.8906\n",
            "Epoch 8/150\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8478\n",
            "Epoch 9/150\n",
            "105/105 [==============================] - 0s 125us/step - loss: 0.8197\n",
            "Epoch 10/150\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.7796\n",
            "Epoch 11/150\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.7623\n",
            "Epoch 12/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.7435\n",
            "Epoch 13/150\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.7158\n",
            "Epoch 14/150\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.6997\n",
            "Epoch 15/150\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.6787\n",
            "Epoch 16/150\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.6761\n",
            "Epoch 17/150\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.6580\n",
            "Epoch 18/150\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.6336\n",
            "Epoch 19/150\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.6277\n",
            "Epoch 20/150\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.6168\n",
            "Epoch 21/150\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.6037\n",
            "Epoch 22/150\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.5966\n",
            "Epoch 23/150\n",
            "105/105 [==============================] - 0s 231us/step - loss: 0.5967\n",
            "Epoch 24/150\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.5867\n",
            "Epoch 25/150\n",
            "105/105 [==============================] - 0s 118us/step - loss: 0.5732\n",
            "Epoch 26/150\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.5647\n",
            "Epoch 27/150\n",
            "105/105 [==============================] - 0s 117us/step - loss: 0.5611\n",
            "Epoch 28/150\n",
            "105/105 [==============================] - 0s 124us/step - loss: 0.5504\n",
            "Epoch 29/150\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.5562\n",
            "Epoch 30/150\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.5514\n",
            "Epoch 31/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5297\n",
            "Epoch 32/150\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.5303\n",
            "Epoch 33/150\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.5236\n",
            "Epoch 34/150\n",
            "105/105 [==============================] - 0s 104us/step - loss: 0.5166\n",
            "Epoch 35/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5219\n",
            "Epoch 36/150\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5169\n",
            "Epoch 37/150\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.5105\n",
            "Epoch 38/150\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4993\n",
            "Epoch 39/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.4998\n",
            "Epoch 40/150\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.4927\n",
            "Epoch 41/150\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.4900\n",
            "Epoch 42/150\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.4875\n",
            "Epoch 43/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4820\n",
            "Epoch 44/150\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.4866\n",
            "Epoch 45/150\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4777\n",
            "Epoch 46/150\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.4729\n",
            "Epoch 47/150\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.4697\n",
            "Epoch 48/150\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4670\n",
            "Epoch 49/150\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.4675\n",
            "Epoch 50/150\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.4577\n",
            "Epoch 51/150\n",
            "105/105 [==============================] - 0s 120us/step - loss: 0.4588\n",
            "Epoch 52/150\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4670\n",
            "Epoch 53/150\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.4493\n",
            "Epoch 54/150\n",
            "105/105 [==============================] - 0s 124us/step - loss: 0.4538\n",
            "Epoch 55/150\n",
            "105/105 [==============================] - 0s 117us/step - loss: 0.4431\n",
            "Epoch 56/150\n",
            "105/105 [==============================] - 0s 119us/step - loss: 0.4498\n",
            "Epoch 57/150\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.4393\n",
            "Epoch 58/150\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.4458\n",
            "Epoch 59/150\n",
            "105/105 [==============================] - 0s 118us/step - loss: 0.4371\n",
            "Epoch 60/150\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.4359\n",
            "Epoch 61/150\n",
            "105/105 [==============================] - 0s 110us/step - loss: 0.4267\n",
            "Epoch 62/150\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.4293\n",
            "Epoch 63/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4261\n",
            "Epoch 64/150\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.4213\n",
            "Epoch 65/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4216\n",
            "Epoch 66/150\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.4206\n",
            "Epoch 67/150\n",
            "105/105 [==============================] - 0s 124us/step - loss: 0.4199\n",
            "Epoch 68/150\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.4120\n",
            "Epoch 69/150\n",
            "105/105 [==============================] - 0s 129us/step - loss: 0.4124\n",
            "Epoch 70/150\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.4074\n",
            "Epoch 71/150\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.4157\n",
            "Epoch 72/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4070\n",
            "Epoch 73/150\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4008\n",
            "Epoch 74/150\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.4005\n",
            "Epoch 75/150\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.3985\n",
            "Epoch 76/150\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.4009\n",
            "Epoch 77/150\n",
            "105/105 [==============================] - 0s 119us/step - loss: 0.3940\n",
            "Epoch 78/150\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4068\n",
            "Epoch 79/150\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.3919\n",
            "Epoch 80/150\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.3864\n",
            "Epoch 81/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.3940\n",
            "Epoch 82/150\n",
            "105/105 [==============================] - 0s 129us/step - loss: 0.3821\n",
            "Epoch 83/150\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.3852\n",
            "Epoch 84/150\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.3875\n",
            "Epoch 85/150\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.3795\n",
            "Epoch 86/150\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.3768\n",
            "Epoch 87/150\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.3791\n",
            "Epoch 88/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.3678\n",
            "Epoch 89/150\n",
            "105/105 [==============================] - 0s 129us/step - loss: 0.3729\n",
            "Epoch 90/150\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.3708\n",
            "Epoch 91/150\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.3712\n",
            "Epoch 92/150\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.3701\n",
            "Epoch 93/150\n",
            "105/105 [==============================] - 0s 120us/step - loss: 0.3696\n",
            "Epoch 94/150\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.3624\n",
            "Epoch 95/150\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.3626\n",
            "Epoch 96/150\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.3619\n",
            "Epoch 97/150\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.3644\n",
            "Epoch 98/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.3638\n",
            "Epoch 99/150\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.3582\n",
            "Epoch 100/150\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.3554\n",
            "Epoch 101/150\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.3540\n",
            "Epoch 102/150\n",
            "105/105 [==============================] - 0s 118us/step - loss: 0.3594\n",
            "Epoch 103/150\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.3512\n",
            "Epoch 104/150\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.3582\n",
            "Epoch 105/150\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.3486\n",
            "Epoch 106/150\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.3514\n",
            "Epoch 107/150\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.3505\n",
            "Epoch 108/150\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.3509\n",
            "Epoch 109/150\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.3409\n",
            "Epoch 110/150\n",
            "105/105 [==============================] - 0s 121us/step - loss: 0.3413\n",
            "Epoch 111/150\n",
            "105/105 [==============================] - 0s 123us/step - loss: 0.3376\n",
            "Epoch 112/150\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.3414\n",
            "Epoch 113/150\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.3404\n",
            "Epoch 114/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.3377\n",
            "Epoch 115/150\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.3403\n",
            "Epoch 116/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.3348\n",
            "Epoch 117/150\n",
            "105/105 [==============================] - 0s 122us/step - loss: 0.3317\n",
            "Epoch 118/150\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.3329\n",
            "Epoch 119/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.3297\n",
            "Epoch 120/150\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.3314\n",
            "Epoch 121/150\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.3325\n",
            "Epoch 122/150\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.3305\n",
            "Epoch 123/150\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.3263\n",
            "Epoch 124/150\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.3269\n",
            "Epoch 125/150\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.3275\n",
            "Epoch 126/150\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.3258\n",
            "Epoch 127/150\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.3293\n",
            "Epoch 128/150\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.3268\n",
            "Epoch 129/150\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.3185\n",
            "Epoch 130/150\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.3212\n",
            "Epoch 131/150\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.3149\n",
            "Epoch 132/150\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.3174\n",
            "Epoch 133/150\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.3158\n",
            "Epoch 134/150\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.3132\n",
            "Epoch 135/150\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.3142\n",
            "Epoch 136/150\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.3091\n",
            "Epoch 137/150\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.3099\n",
            "Epoch 138/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.3123\n",
            "Epoch 139/150\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.3098\n",
            "Epoch 140/150\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.3088\n",
            "Epoch 141/150\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.3093\n",
            "Epoch 142/150\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.3079\n",
            "Epoch 143/150\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.3040\n",
            "Epoch 144/150\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.3049\n",
            "Epoch 145/150\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.3010\n",
            "Epoch 146/150\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.3064\n",
            "Epoch 147/150\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.2998\n",
            "Epoch 148/150\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.2997\n",
            "Epoch 149/150\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.2980\n",
            "Epoch 150/150\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.3011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXyD2AuGZfKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "928e3280-ea6e-4b3d-e358-3fad9dc83e59"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(hist.history['loss'])"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f804a6f3ed0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y-SgSSdRcih5"
      },
      "source": [
        "### Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBgKZkhkcih6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "outputId": "946267c5-05a9-4327-f61b-9a782e54c715"
      },
      "source": [
        "model.predict(X_test)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.2708594e-02, 6.9011241e-01, 2.4717899e-01],\n",
              "       [1.2318341e-03, 4.8754233e-01, 5.1122588e-01],\n",
              "       [3.4244405e-03, 9.7752228e-02, 8.9882338e-01],\n",
              "       [9.4049311e-01, 5.8867913e-02, 6.3899049e-04],\n",
              "       [3.0899667e-03, 1.8323639e-01, 8.1367362e-01],\n",
              "       [1.0672783e-01, 6.7434305e-01, 2.1892907e-01],\n",
              "       [9.0274382e-01, 9.6078709e-02, 1.1774689e-03],\n",
              "       [2.7374571e-02, 4.4919580e-01, 5.2342957e-01],\n",
              "       [8.5778183e-01, 1.4043272e-01, 1.7855327e-03],\n",
              "       [3.2502588e-02, 5.6068945e-01, 4.0680799e-01],\n",
              "       [3.3318840e-02, 5.7320118e-01, 3.9347997e-01],\n",
              "       [6.2143174e-03, 5.3384972e-01, 4.5993596e-01],\n",
              "       [1.4544571e-03, 2.4087958e-01, 7.5766599e-01],\n",
              "       [5.3510052e-04, 3.5639003e-01, 6.4307487e-01],\n",
              "       [9.3620467e-01, 6.2962063e-02, 8.3339936e-04],\n",
              "       [9.1894501e-01, 7.9536855e-02, 1.5181222e-03],\n",
              "       [7.5529455e-03, 3.4188670e-01, 6.5056032e-01],\n",
              "       [6.6294370e-04, 2.9723760e-01, 7.0209950e-01],\n",
              "       [9.6740144e-01, 3.2312129e-02, 2.8632666e-04],\n",
              "       [8.9147919e-01, 1.0746389e-01, 1.0569135e-03],\n",
              "       [6.5682620e-02, 6.7255902e-01, 2.6175836e-01],\n",
              "       [1.5463086e-03, 1.8655774e-01, 8.1189597e-01],\n",
              "       [9.5892668e-01, 4.0864598e-02, 2.0876122e-04],\n",
              "       [5.1538111e-03, 4.6566787e-01, 5.2917826e-01],\n",
              "       [7.3064022e-02, 6.7774457e-01, 2.4919137e-01],\n",
              "       [9.2573995e-03, 2.1651657e-01, 7.7422607e-01],\n",
              "       [4.2655118e-02, 7.0803678e-01, 2.4930809e-01],\n",
              "       [4.9772706e-02, 6.7207754e-01, 2.7814984e-01],\n",
              "       [1.8031115e-02, 6.3043052e-01, 3.5153839e-01],\n",
              "       [1.4238099e-03, 1.5187071e-01, 8.4670556e-01],\n",
              "       [9.3479282e-01, 6.4715602e-02, 4.9160223e-04],\n",
              "       [2.3364553e-02, 5.3906369e-01, 4.3757179e-01],\n",
              "       [1.0503002e-01, 7.3298305e-01, 1.6198683e-01],\n",
              "       [9.5898587e-01, 4.0811308e-02, 2.0279494e-04],\n",
              "       [3.4599166e-02, 7.3755264e-01, 2.2784826e-01],\n",
              "       [9.3794638e-01, 6.1532203e-02, 5.2137254e-04],\n",
              "       [9.5636529e-01, 4.2804904e-02, 8.2983822e-04],\n",
              "       [8.9206028e-04, 1.7642631e-01, 8.2268161e-01],\n",
              "       [9.6696657e-01, 3.2759178e-02, 2.7422962e-04],\n",
              "       [7.4879089e-03, 1.9411591e-01, 7.9839617e-01],\n",
              "       [4.0256488e-03, 2.4986000e-01, 7.4611437e-01],\n",
              "       [4.0640891e-02, 6.9973737e-01, 2.5962174e-01],\n",
              "       [9.5937765e-01, 4.0283650e-02, 3.3875534e-04],\n",
              "       [9.5658964e-01, 4.3195430e-02, 2.1496946e-04],\n",
              "       [1.9890092e-02, 5.7865399e-01, 4.0145588e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P32ASP1Vjt0a"
      },
      "source": [
        "### Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n8rd0jjAjyTR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "5dd2b3b1-7fe9-43d8-a81e-937b5972b1d7"
      },
      "source": [
        "scores = model.evaluate(X_test, y_test)\n",
        "print(\"score=\", scores)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45/45 [==============================] - 0s 702us/step\n",
            "('score=', 0.29318226642078826)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgDv1Jp1Zxi8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"iris.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XiipRpe7rbVh"
      },
      "source": [
        "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
        "\n",
        "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v5Du3lubr4sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0a0073e-53b4-4da2-9c85-7610dbcd4d8d"
      },
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(3, input_dim=4, activation='softmax'))\n",
        "model2.add(Dense(3, activation='softmax'))\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='sgd')\n",
        "hist = model2.fit(X_train, y_train, epochs=600, batch_size=10)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.1216\n",
            "Epoch 2/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 1.0916\n",
            "Epoch 3/600\n",
            "105/105 [==============================] - 0s 161us/step - loss: 1.0572\n",
            "Epoch 4/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 1.0239\n",
            "Epoch 5/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 1.0043\n",
            "Epoch 6/600\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.9855\n",
            "Epoch 7/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.9715\n",
            "Epoch 8/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.9560\n",
            "Epoch 9/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.9407\n",
            "Epoch 10/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.9286\n",
            "Epoch 11/600\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.9171\n",
            "Epoch 12/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.9061\n",
            "Epoch 13/600\n",
            "105/105 [==============================] - 0s 123us/step - loss: 0.8938\n",
            "Epoch 14/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.8832\n",
            "Epoch 15/600\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.8726\n",
            "Epoch 16/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.8634\n",
            "Epoch 17/600\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.8548\n",
            "Epoch 18/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.8449\n",
            "Epoch 19/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.8367\n",
            "Epoch 20/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.8282\n",
            "Epoch 21/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.8211\n",
            "Epoch 22/600\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.8132\n",
            "Epoch 23/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.8061\n",
            "Epoch 24/600\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.7986\n",
            "Epoch 25/600\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.7923\n",
            "Epoch 26/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.7855\n",
            "Epoch 27/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.7788\n",
            "Epoch 28/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.7726\n",
            "Epoch 29/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.7671\n",
            "Epoch 30/600\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.7619\n",
            "Epoch 31/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.7558\n",
            "Epoch 32/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.7501\n",
            "Epoch 33/600\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.7448\n",
            "Epoch 34/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.7399\n",
            "Epoch 35/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.7354\n",
            "Epoch 36/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.7305\n",
            "Epoch 37/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.7263\n",
            "Epoch 38/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.7218\n",
            "Epoch 39/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.7173\n",
            "Epoch 40/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.7135\n",
            "Epoch 41/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.7092\n",
            "Epoch 42/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.7053\n",
            "Epoch 43/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.7016\n",
            "Epoch 44/600\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.6982\n",
            "Epoch 45/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.6942\n",
            "Epoch 46/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.6910\n",
            "Epoch 47/600\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.6874\n",
            "Epoch 48/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.6842\n",
            "Epoch 49/600\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.6808\n",
            "Epoch 50/600\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.6781\n",
            "Epoch 51/600\n",
            "105/105 [==============================] - 0s 123us/step - loss: 0.6749\n",
            "Epoch 52/600\n",
            "105/105 [==============================] - 0s 114us/step - loss: 0.6718\n",
            "Epoch 53/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.6689\n",
            "Epoch 54/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.6661\n",
            "Epoch 55/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.6632\n",
            "Epoch 56/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.6605\n",
            "Epoch 57/600\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.6580\n",
            "Epoch 58/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.6554\n",
            "Epoch 59/600\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.6532\n",
            "Epoch 60/600\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.6502\n",
            "Epoch 61/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.6479\n",
            "Epoch 62/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.6459\n",
            "Epoch 63/600\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.6433\n",
            "Epoch 64/600\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.6410\n",
            "Epoch 65/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.6390\n",
            "Epoch 66/600\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.6371\n",
            "Epoch 67/600\n",
            "105/105 [==============================] - 0s 122us/step - loss: 0.6348\n",
            "Epoch 68/600\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.6331\n",
            "Epoch 69/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.6309\n",
            "Epoch 70/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.6287\n",
            "Epoch 71/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.6269\n",
            "Epoch 72/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.6250\n",
            "Epoch 73/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.6231\n",
            "Epoch 74/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.6217\n",
            "Epoch 75/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.6195\n",
            "Epoch 76/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.6179\n",
            "Epoch 77/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.6162\n",
            "Epoch 78/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.6146\n",
            "Epoch 79/600\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.6129\n",
            "Epoch 80/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.6113\n",
            "Epoch 81/600\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.6098\n",
            "Epoch 82/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.6082\n",
            "Epoch 83/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.6066\n",
            "Epoch 84/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.6054\n",
            "Epoch 85/600\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.6037\n",
            "Epoch 86/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.6025\n",
            "Epoch 87/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.6009\n",
            "Epoch 88/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5998\n",
            "Epoch 89/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.5983\n",
            "Epoch 90/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5970\n",
            "Epoch 91/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.5957\n",
            "Epoch 92/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.5943\n",
            "Epoch 93/600\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.5933\n",
            "Epoch 94/600\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.5919\n",
            "Epoch 95/600\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.5908\n",
            "Epoch 96/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.5894\n",
            "Epoch 97/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.5885\n",
            "Epoch 98/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.5872\n",
            "Epoch 99/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.5861\n",
            "Epoch 100/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.5851\n",
            "Epoch 101/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5836\n",
            "Epoch 102/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.5829\n",
            "Epoch 103/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5817\n",
            "Epoch 104/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.5807\n",
            "Epoch 105/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5797\n",
            "Epoch 106/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.5787\n",
            "Epoch 107/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.5775\n",
            "Epoch 108/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.5768\n",
            "Epoch 109/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.5759\n",
            "Epoch 110/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.5748\n",
            "Epoch 111/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5738\n",
            "Epoch 112/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5730\n",
            "Epoch 113/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.5720\n",
            "Epoch 114/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5715\n",
            "Epoch 115/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.5703\n",
            "Epoch 116/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.5694\n",
            "Epoch 117/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.5686\n",
            "Epoch 118/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.5677\n",
            "Epoch 119/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.5670\n",
            "Epoch 120/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5660\n",
            "Epoch 121/600\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.5654\n",
            "Epoch 122/600\n",
            "105/105 [==============================] - 0s 203us/step - loss: 0.5647\n",
            "Epoch 123/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.5637\n",
            "Epoch 124/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.5631\n",
            "Epoch 125/600\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.5625\n",
            "Epoch 126/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.5615\n",
            "Epoch 127/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.5607\n",
            "Epoch 128/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5601\n",
            "Epoch 129/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.5593\n",
            "Epoch 130/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5585\n",
            "Epoch 131/600\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.5578\n",
            "Epoch 132/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.5577\n",
            "Epoch 133/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5566\n",
            "Epoch 134/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5558\n",
            "Epoch 135/600\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.5553\n",
            "Epoch 136/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5546\n",
            "Epoch 137/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.5542\n",
            "Epoch 138/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.5534\n",
            "Epoch 139/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5526\n",
            "Epoch 140/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.5520\n",
            "Epoch 141/600\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.5515\n",
            "Epoch 142/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.5507\n",
            "Epoch 143/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5502\n",
            "Epoch 144/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.5498\n",
            "Epoch 145/600\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.5491\n",
            "Epoch 146/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.5486\n",
            "Epoch 147/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5479\n",
            "Epoch 148/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.5473\n",
            "Epoch 149/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5468\n",
            "Epoch 150/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.5464\n",
            "Epoch 151/600\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.5460\n",
            "Epoch 152/600\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.5451\n",
            "Epoch 153/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.5447\n",
            "Epoch 154/600\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.5442\n",
            "Epoch 155/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.5437\n",
            "Epoch 156/600\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.5431\n",
            "Epoch 157/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.5427\n",
            "Epoch 158/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5425\n",
            "Epoch 159/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5417\n",
            "Epoch 160/600\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.5410\n",
            "Epoch 161/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.5406\n",
            "Epoch 162/600\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.5402\n",
            "Epoch 163/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.5398\n",
            "Epoch 164/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5392\n",
            "Epoch 165/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.5390\n",
            "Epoch 166/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5384\n",
            "Epoch 167/600\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.5378\n",
            "Epoch 168/600\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.5374\n",
            "Epoch 169/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.5370\n",
            "Epoch 170/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5366\n",
            "Epoch 171/600\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.5366\n",
            "Epoch 172/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5357\n",
            "Epoch 173/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.5353\n",
            "Epoch 174/600\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.5349\n",
            "Epoch 175/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.5344\n",
            "Epoch 176/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5339\n",
            "Epoch 177/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.5337\n",
            "Epoch 178/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5333\n",
            "Epoch 179/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5329\n",
            "Epoch 180/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.5324\n",
            "Epoch 181/600\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.5322\n",
            "Epoch 182/600\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.5317\n",
            "Epoch 183/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.5315\n",
            "Epoch 184/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.5309\n",
            "Epoch 185/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5304\n",
            "Epoch 186/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.5302\n",
            "Epoch 187/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.5301\n",
            "Epoch 188/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5294\n",
            "Epoch 189/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.5292\n",
            "Epoch 190/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.5287\n",
            "Epoch 191/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5284\n",
            "Epoch 192/600\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.5282\n",
            "Epoch 193/600\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.5277\n",
            "Epoch 194/600\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.5280\n",
            "Epoch 195/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.5271\n",
            "Epoch 196/600\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.5268\n",
            "Epoch 197/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5265\n",
            "Epoch 198/600\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.5263\n",
            "Epoch 199/600\n",
            "105/105 [==============================] - 0s 200us/step - loss: 0.5258\n",
            "Epoch 200/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.5256\n",
            "Epoch 201/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5253\n",
            "Epoch 202/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.5248\n",
            "Epoch 203/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.5248\n",
            "Epoch 204/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.5242\n",
            "Epoch 205/600\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.5239\n",
            "Epoch 206/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.5235\n",
            "Epoch 207/600\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.5234\n",
            "Epoch 208/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5229\n",
            "Epoch 209/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.5227\n",
            "Epoch 210/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.5224\n",
            "Epoch 211/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.5221\n",
            "Epoch 212/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5221\n",
            "Epoch 213/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.5217\n",
            "Epoch 214/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5213\n",
            "Epoch 215/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.5210\n",
            "Epoch 216/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.5208\n",
            "Epoch 217/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5204\n",
            "Epoch 218/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.5200\n",
            "Epoch 219/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.5197\n",
            "Epoch 220/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5199\n",
            "Epoch 221/600\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.5196\n",
            "Epoch 222/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5190\n",
            "Epoch 223/600\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.5189\n",
            "Epoch 224/600\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.5186\n",
            "Epoch 225/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5183\n",
            "Epoch 226/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.5180\n",
            "Epoch 227/600\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.5177\n",
            "Epoch 228/600\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.5177\n",
            "Epoch 229/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.5172\n",
            "Epoch 230/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.5170\n",
            "Epoch 231/600\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.5170\n",
            "Epoch 232/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5166\n",
            "Epoch 233/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.5161\n",
            "Epoch 234/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.5160\n",
            "Epoch 235/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.5159\n",
            "Epoch 236/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5155\n",
            "Epoch 237/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.5152\n",
            "Epoch 238/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.5151\n",
            "Epoch 239/600\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.5149\n",
            "Epoch 240/600\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.5147\n",
            "Epoch 241/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5145\n",
            "Epoch 242/600\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.5142\n",
            "Epoch 243/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.5140\n",
            "Epoch 244/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5139\n",
            "Epoch 245/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.5135\n",
            "Epoch 246/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.5134\n",
            "Epoch 247/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.5134\n",
            "Epoch 248/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.5128\n",
            "Epoch 249/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.5127\n",
            "Epoch 250/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.5126\n",
            "Epoch 251/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5122\n",
            "Epoch 252/600\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.5120\n",
            "Epoch 253/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5118\n",
            "Epoch 254/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.5116\n",
            "Epoch 255/600\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.5113\n",
            "Epoch 256/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.5112\n",
            "Epoch 257/600\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.5109\n",
            "Epoch 258/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5109\n",
            "Epoch 259/600\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.5105\n",
            "Epoch 260/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.5104\n",
            "Epoch 261/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.5103\n",
            "Epoch 262/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.5101\n",
            "Epoch 263/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5098\n",
            "Epoch 264/600\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.5095\n",
            "Epoch 265/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.5094\n",
            "Epoch 266/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.5093\n",
            "Epoch 267/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.5091\n",
            "Epoch 268/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5089\n",
            "Epoch 269/600\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.5088\n",
            "Epoch 270/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.5086\n",
            "Epoch 271/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.5083\n",
            "Epoch 272/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.5082\n",
            "Epoch 273/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.5080\n",
            "Epoch 274/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5079\n",
            "Epoch 275/600\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.5074\n",
            "Epoch 276/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5074\n",
            "Epoch 277/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.5072\n",
            "Epoch 278/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.5070\n",
            "Epoch 279/600\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.5070\n",
            "Epoch 280/600\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.5068\n",
            "Epoch 281/600\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.5069\n",
            "Epoch 282/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5064\n",
            "Epoch 283/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.5063\n",
            "Epoch 284/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.5062\n",
            "Epoch 285/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.5059\n",
            "Epoch 286/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5057\n",
            "Epoch 287/600\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.5055\n",
            "Epoch 288/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.5055\n",
            "Epoch 289/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.5052\n",
            "Epoch 290/600\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.5051\n",
            "Epoch 291/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.5049\n",
            "Epoch 292/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5046\n",
            "Epoch 293/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5044\n",
            "Epoch 294/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.5049\n",
            "Epoch 295/600\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.5044\n",
            "Epoch 296/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.5046\n",
            "Epoch 297/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.5039\n",
            "Epoch 298/600\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.5038\n",
            "Epoch 299/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5038\n",
            "Epoch 300/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.5036\n",
            "Epoch 301/600\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.5033\n",
            "Epoch 302/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.5033\n",
            "Epoch 303/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.5031\n",
            "Epoch 304/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.5030\n",
            "Epoch 305/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.5028\n",
            "Epoch 306/600\n",
            "105/105 [==============================] - 0s 246us/step - loss: 0.5029\n",
            "Epoch 307/600\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.5027\n",
            "Epoch 308/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.5024\n",
            "Epoch 309/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.5022\n",
            "Epoch 310/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.5022\n",
            "Epoch 311/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.5018\n",
            "Epoch 312/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.5021\n",
            "Epoch 313/600\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.5016\n",
            "Epoch 314/600\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.5016\n",
            "Epoch 315/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.5012\n",
            "Epoch 316/600\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.5012\n",
            "Epoch 317/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.5009\n",
            "Epoch 318/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5009\n",
            "Epoch 319/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.5006\n",
            "Epoch 320/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5009\n",
            "Epoch 321/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.5004\n",
            "Epoch 322/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.5003\n",
            "Epoch 323/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5002\n",
            "Epoch 324/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4999\n",
            "Epoch 325/600\n",
            "105/105 [==============================] - 0s 214us/step - loss: 0.4999\n",
            "Epoch 326/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.4998\n",
            "Epoch 327/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4997\n",
            "Epoch 328/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4996\n",
            "Epoch 329/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.4995\n",
            "Epoch 330/600\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4993\n",
            "Epoch 331/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.4991\n",
            "Epoch 332/600\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.4988\n",
            "Epoch 333/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4989\n",
            "Epoch 334/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4987\n",
            "Epoch 335/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.4987\n",
            "Epoch 336/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4986\n",
            "Epoch 337/600\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.4983\n",
            "Epoch 338/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.4985\n",
            "Epoch 339/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4980\n",
            "Epoch 340/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4979\n",
            "Epoch 341/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4978\n",
            "Epoch 342/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.4979\n",
            "Epoch 343/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4978\n",
            "Epoch 344/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4975\n",
            "Epoch 345/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4975\n",
            "Epoch 346/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4972\n",
            "Epoch 347/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.4974\n",
            "Epoch 348/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4970\n",
            "Epoch 349/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4968\n",
            "Epoch 350/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.4969\n",
            "Epoch 351/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4965\n",
            "Epoch 352/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4967\n",
            "Epoch 353/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.4965\n",
            "Epoch 354/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4965\n",
            "Epoch 355/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.4965\n",
            "Epoch 356/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4960\n",
            "Epoch 357/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4963\n",
            "Epoch 358/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4961\n",
            "Epoch 359/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4956\n",
            "Epoch 360/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4956\n",
            "Epoch 361/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.4956\n",
            "Epoch 362/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.4956\n",
            "Epoch 363/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4955\n",
            "Epoch 364/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4953\n",
            "Epoch 365/600\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.4952\n",
            "Epoch 366/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4952\n",
            "Epoch 367/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4949\n",
            "Epoch 368/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4948\n",
            "Epoch 369/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4947\n",
            "Epoch 370/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4945\n",
            "Epoch 371/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4945\n",
            "Epoch 372/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4944\n",
            "Epoch 373/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4942\n",
            "Epoch 374/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4941\n",
            "Epoch 375/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.4941\n",
            "Epoch 376/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4940\n",
            "Epoch 377/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.4938\n",
            "Epoch 378/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4937\n",
            "Epoch 379/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4937\n",
            "Epoch 380/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4934\n",
            "Epoch 381/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.4933\n",
            "Epoch 382/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4932\n",
            "Epoch 383/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4931\n",
            "Epoch 384/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.4931\n",
            "Epoch 385/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4927\n",
            "Epoch 386/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.4927\n",
            "Epoch 387/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4926\n",
            "Epoch 388/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4927\n",
            "Epoch 389/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4925\n",
            "Epoch 390/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4924\n",
            "Epoch 391/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.4922\n",
            "Epoch 392/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.4921\n",
            "Epoch 393/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.4922\n",
            "Epoch 394/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4922\n",
            "Epoch 395/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.4919\n",
            "Epoch 396/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.4918\n",
            "Epoch 397/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4917\n",
            "Epoch 398/600\n",
            "105/105 [==============================] - 0s 206us/step - loss: 0.4915\n",
            "Epoch 399/600\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.4915\n",
            "Epoch 400/600\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.4916\n",
            "Epoch 401/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4915\n",
            "Epoch 402/600\n",
            "105/105 [==============================] - 0s 121us/step - loss: 0.4912\n",
            "Epoch 403/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4911\n",
            "Epoch 404/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4910\n",
            "Epoch 405/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.4910\n",
            "Epoch 406/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4909\n",
            "Epoch 407/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4906\n",
            "Epoch 408/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.4904\n",
            "Epoch 409/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4904\n",
            "Epoch 410/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.4905\n",
            "Epoch 411/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4901\n",
            "Epoch 412/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4900\n",
            "Epoch 413/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4903\n",
            "Epoch 414/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4897\n",
            "Epoch 415/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4900\n",
            "Epoch 416/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4897\n",
            "Epoch 417/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4897\n",
            "Epoch 418/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4894\n",
            "Epoch 419/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.4893\n",
            "Epoch 420/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.4894\n",
            "Epoch 421/600\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.4892\n",
            "Epoch 422/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4890\n",
            "Epoch 423/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.4889\n",
            "Epoch 424/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4891\n",
            "Epoch 425/600\n",
            "105/105 [==============================] - 0s 208us/step - loss: 0.4889\n",
            "Epoch 426/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4888\n",
            "Epoch 427/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.4886\n",
            "Epoch 428/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.4884\n",
            "Epoch 429/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4883\n",
            "Epoch 430/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4883\n",
            "Epoch 431/600\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.4883\n",
            "Epoch 432/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.4884\n",
            "Epoch 433/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4880\n",
            "Epoch 434/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4879\n",
            "Epoch 435/600\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.4876\n",
            "Epoch 436/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4879\n",
            "Epoch 437/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4875\n",
            "Epoch 438/600\n",
            "105/105 [==============================] - 0s 119us/step - loss: 0.4875\n",
            "Epoch 439/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4872\n",
            "Epoch 440/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.4873\n",
            "Epoch 441/600\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.4871\n",
            "Epoch 442/600\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.4871\n",
            "Epoch 443/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.4868\n",
            "Epoch 444/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4867\n",
            "Epoch 445/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4869\n",
            "Epoch 446/600\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.4868\n",
            "Epoch 447/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.4867\n",
            "Epoch 448/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4862\n",
            "Epoch 449/600\n",
            "105/105 [==============================] - 0s 222us/step - loss: 0.4863\n",
            "Epoch 450/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4862\n",
            "Epoch 451/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.4862\n",
            "Epoch 452/600\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.4858\n",
            "Epoch 453/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4858\n",
            "Epoch 454/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4856\n",
            "Epoch 455/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4857\n",
            "Epoch 456/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.4855\n",
            "Epoch 457/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4854\n",
            "Epoch 458/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4854\n",
            "Epoch 459/600\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.4852\n",
            "Epoch 460/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.4849\n",
            "Epoch 461/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4850\n",
            "Epoch 462/600\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.4848\n",
            "Epoch 463/600\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.4849\n",
            "Epoch 464/600\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.4847\n",
            "Epoch 465/600\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.4844\n",
            "Epoch 466/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.4842\n",
            "Epoch 467/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.4843\n",
            "Epoch 468/600\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.4841\n",
            "Epoch 469/600\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.4841\n",
            "Epoch 470/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4838\n",
            "Epoch 471/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4837\n",
            "Epoch 472/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.4835\n",
            "Epoch 473/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.4835\n",
            "Epoch 474/600\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.4834\n",
            "Epoch 475/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.4833\n",
            "Epoch 476/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4834\n",
            "Epoch 477/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4831\n",
            "Epoch 478/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.4831\n",
            "Epoch 479/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.4828\n",
            "Epoch 480/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4826\n",
            "Epoch 481/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.4829\n",
            "Epoch 482/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4826\n",
            "Epoch 483/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.4824\n",
            "Epoch 484/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.4823\n",
            "Epoch 485/600\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.4822\n",
            "Epoch 486/600\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.4821\n",
            "Epoch 487/600\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.4817\n",
            "Epoch 488/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4815\n",
            "Epoch 489/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.4816\n",
            "Epoch 490/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.4812\n",
            "Epoch 491/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4813\n",
            "Epoch 492/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4812\n",
            "Epoch 493/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.4810\n",
            "Epoch 494/600\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4809\n",
            "Epoch 495/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4807\n",
            "Epoch 496/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.4806\n",
            "Epoch 497/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.4804\n",
            "Epoch 498/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.4802\n",
            "Epoch 499/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4801\n",
            "Epoch 500/600\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.4800\n",
            "Epoch 501/600\n",
            "105/105 [==============================] - 0s 203us/step - loss: 0.4799\n",
            "Epoch 502/600\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.4797\n",
            "Epoch 503/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4797\n",
            "Epoch 504/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4797\n",
            "Epoch 505/600\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.4793\n",
            "Epoch 506/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.4792\n",
            "Epoch 507/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4790\n",
            "Epoch 508/600\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.4789\n",
            "Epoch 509/600\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4787\n",
            "Epoch 510/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4785\n",
            "Epoch 511/600\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.4784\n",
            "Epoch 512/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4784\n",
            "Epoch 513/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.4780\n",
            "Epoch 514/600\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.4780\n",
            "Epoch 515/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.4779\n",
            "Epoch 516/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4777\n",
            "Epoch 517/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4773\n",
            "Epoch 518/600\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4774\n",
            "Epoch 519/600\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.4771\n",
            "Epoch 520/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4769\n",
            "Epoch 521/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4767\n",
            "Epoch 522/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.4766\n",
            "Epoch 523/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4766\n",
            "Epoch 524/600\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.4765\n",
            "Epoch 525/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4760\n",
            "Epoch 526/600\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.4760\n",
            "Epoch 527/600\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.4758\n",
            "Epoch 528/600\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.4754\n",
            "Epoch 529/600\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.4754\n",
            "Epoch 530/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4752\n",
            "Epoch 531/600\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.4754\n",
            "Epoch 532/600\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.4750\n",
            "Epoch 533/600\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4749\n",
            "Epoch 534/600\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.4744\n",
            "Epoch 535/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4744\n",
            "Epoch 536/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4742\n",
            "Epoch 537/600\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.4739\n",
            "Epoch 538/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4737\n",
            "Epoch 539/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4736\n",
            "Epoch 540/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.4735\n",
            "Epoch 541/600\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.4733\n",
            "Epoch 542/600\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.4731\n",
            "Epoch 543/600\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.4729\n",
            "Epoch 544/600\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.4727\n",
            "Epoch 545/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4725\n",
            "Epoch 546/600\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.4724\n",
            "Epoch 547/600\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.4720\n",
            "Epoch 548/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4720\n",
            "Epoch 549/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4714\n",
            "Epoch 550/600\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.4712\n",
            "Epoch 551/600\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.4710\n",
            "Epoch 552/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.4709\n",
            "Epoch 553/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.4710\n",
            "Epoch 554/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4706\n",
            "Epoch 555/600\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.4703\n",
            "Epoch 556/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.4701\n",
            "Epoch 557/600\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.4699\n",
            "Epoch 558/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4696\n",
            "Epoch 559/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4694\n",
            "Epoch 560/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4692\n",
            "Epoch 561/600\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.4689\n",
            "Epoch 562/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4686\n",
            "Epoch 563/600\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.4686\n",
            "Epoch 564/600\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.4682\n",
            "Epoch 565/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4681\n",
            "Epoch 566/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4677\n",
            "Epoch 567/600\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.4675\n",
            "Epoch 568/600\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.4673\n",
            "Epoch 569/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4675\n",
            "Epoch 570/600\n",
            "105/105 [==============================] - 0s 202us/step - loss: 0.4666\n",
            "Epoch 571/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4666\n",
            "Epoch 572/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4665\n",
            "Epoch 573/600\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.4659\n",
            "Epoch 574/600\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.4657\n",
            "Epoch 575/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4656\n",
            "Epoch 576/600\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.4651\n",
            "Epoch 577/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.4650\n",
            "Epoch 578/600\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.4647\n",
            "Epoch 579/600\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.4643\n",
            "Epoch 580/600\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.4642\n",
            "Epoch 581/600\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.4639\n",
            "Epoch 582/600\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.4636\n",
            "Epoch 583/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.4635\n",
            "Epoch 584/600\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4632\n",
            "Epoch 585/600\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.4627\n",
            "Epoch 586/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.4629\n",
            "Epoch 587/600\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4623\n",
            "Epoch 588/600\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.4621\n",
            "Epoch 589/600\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.4617\n",
            "Epoch 590/600\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.4613\n",
            "Epoch 591/600\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.4611\n",
            "Epoch 592/600\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.4608\n",
            "Epoch 593/600\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4605\n",
            "Epoch 594/600\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.4602\n",
            "Epoch 595/600\n",
            "105/105 [==============================] - 0s 193us/step - loss: 0.4601\n",
            "Epoch 596/600\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.4597\n",
            "Epoch 597/600\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.4594\n",
            "Epoch 598/600\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.4590\n",
            "Epoch 599/600\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.4587\n",
            "Epoch 600/600\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.4585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlrA9GEVZ5tR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "104567e6-d5e1-40ac-a2aa-8d7ba9321f43"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(hist.history['loss'])"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f80474aff10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XElKOTyeaCig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fba75894-ed10-4615-81c5-1f7daca18d8e"
      },
      "source": [
        "scores = model2.evaluate(X_test, y_test)\n",
        "print(\"score=\", scores)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45/45 [==============================] - 0s 1ms/step\n",
            "('score=', 0.46191936863793265)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEgI84eFaG5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}